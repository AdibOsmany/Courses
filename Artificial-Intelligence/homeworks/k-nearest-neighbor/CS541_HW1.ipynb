{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qzwzbGVi3kp"
      },
      "source": [
        "# CS 541-A-Homework 1\n",
        "## K nearest neighbors and distance metrics\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### ***Fill your details below***\n",
        "### Name: Adib Osmany\n",
        "### CWID: 20013762\n",
        "### Email ID: aosmany@stevens.edu\n",
        "### References: ***Cite your references here***\n",
        "\n",
        "\n",
        "---\n",
        "### Submission guidelines:\n",
        "\n",
        "#### 1. Submit this notebook along with its PDF version. You can do this by clicking File->Print->\"Save as PDF\"\n",
        "\n",
        "#### 2. Name the file as \"<mailID_HWnumber.extension>\". For example, mailID is abcdefg @stevens.edu then name the files as abcdefg_HW1.ipynb and abcdefg_HW1.pdf.\n",
        "\n",
        "#### 3. Please do not Zip your files.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biSEuRTKjDYk"
      },
      "source": [
        "### Install prerequisites"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdQyoGii_Wmw",
        "outputId": "82730f35-93e3-4bfd-8ba8-596524e7e518"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.17.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-2kcsy3ob\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-2kcsy3ob\n",
            "  Resolved https://github.com/huggingface/transformers to commit c8d98405a8f7b0e5d07391b671dcc61bb9d7bad5\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.39.0.dev0) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.39.0.dev0) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.0.dev0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.0.dev0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.0.dev0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.39.0.dev0) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece datasets\n",
        "!pip install git+https://github.com/huggingface/transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt7okMLwjGbs"
      },
      "source": [
        "### Import relevant libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "hERMnB1NAATA"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuuIbvZijJxc"
      },
      "source": [
        "### Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRl7Cje_ASHC",
        "outputId": "1ffc1c50-f815-445a-a7cc-8dc1cbe95fc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train\n",
            " Dataset({\n",
            "    features: ['text', 'label'],\n",
            "    num_rows: 10003\n",
            "})\n",
            "Test\n",
            " Dataset({\n",
            "    features: ['text', 'label'],\n",
            "    num_rows: 3080\n",
            "})\n",
            "Example from the training set:\n",
            "{'text': 'I am still waiting on my card?', 'label': 11}\n"
          ]
        }
      ],
      "source": [
        "# Load Financial banking77 dataset\n",
        "train_dataset, test_dataset = load_dataset('banking77', split=['train', 'test'])\n",
        "print(\"Train\\n\", train_dataset)\n",
        "print(\"Test\\n\", test_dataset)\n",
        "\n",
        "# Accessing example from the training set\n",
        "example = train_dataset[0]\n",
        "print(\"Example from the training set:\")\n",
        "print(example)\n",
        "\n",
        "# Zhaozhuo: Just organize a bit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpylBj2WjNB0"
      },
      "source": [
        "### Load tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oz3uvkTdBCJH"
      },
      "outputs": [],
      "source": [
        "# Define tokenzier\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\", use_fast=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRiMIlcrjO3M"
      },
      "source": [
        "### Tokenize dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "V70L-_EzBcV2"
      },
      "outputs": [],
      "source": [
        "# Tokenize the training and testing set\n",
        "tokenized_trainset=tokenizer(train_dataset[\"text\"])['input_ids']\n",
        "train_labels=train_dataset[\"label\"]\n",
        "\n",
        "tokenized_testset=tokenizer(test_dataset[\"text\"])['input_ids']\n",
        "test_labels=test_dataset[\"label\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPAz1gKDjfvt"
      },
      "source": [
        "## Q1. (40 Points) Write a function to compute Jaccard distance between lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "KwmJrQSXCKta"
      },
      "outputs": [],
      "source": [
        "def jaccard_distance(list1, list2):\n",
        "  '''\n",
        "  Compute the Jaccard distance between two lists.\n",
        "  '''\n",
        "  l1=set(list1)\n",
        "  l2=set(list2)\n",
        "  similarity= l1.intersection(l2)\n",
        "  denominator=l1.union(l2)\n",
        "\n",
        "  return (1-(len(similarity)/len(denominator)))\n",
        "\n",
        "\n",
        "  return #return the Jaccard distance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te9JasqBl9F_"
      },
      "source": [
        "### Test script to check the Jaccard distance calculation function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sji0007Bjny8",
        "outputId": "b7b49f7c-2d23-4af2-b730-252e1f2303f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q1: Correct\n"
          ]
        }
      ],
      "source": [
        "test_distance=jaccard_distance([1,2],[1,3,4])\n",
        "\n",
        "if(test_distance==0.75):\n",
        "  print(\"Q1: Correct\")\n",
        "else:\n",
        "  print(\"Q1: Wrong\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qrzue5k37P4C"
      },
      "source": [
        "## Q2(a) (15 Points) Write code to implement the K nearest neighbors (KNN) algorithm with ***Jaccard*** distance. Use K=1 to compute accuracy on test set.\n",
        "\n",
        "### Note: Please code this section from scratch, do not use KNN implementations from libraries such as scikit-learn.\n",
        "\n",
        "### Hint 1: K=1 means that only one nearest neighbor votes for the sample's label. Find the sample in the trainset which is closest to the test sample and predict it's label as the test sample's label.\n",
        "\n",
        "### Hint 2: Use the tokenized train set and tokenized test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "960grPBpG1ER",
        "outputId": "2ba9af85-0167-417e-ee26-d87987c8d5ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7532467532467533\n"
          ]
        }
      ],
      "source": [
        "total_num_correct = 0\n",
        "number_of_testing_cases = len(tokenized_testset)\n",
        "\n",
        "'''\n",
        "Add KNN code here\n",
        "\n",
        "returns nearest neigbor's label\n",
        "'''\n",
        "def KNN(sample):\n",
        "  results=[]\n",
        "  for i in range(len(tokenized_trainset)):\n",
        "    point=tokenized_trainset[i]\n",
        "    label=train_labels[i]\n",
        "    results.append((jaccard_distance(sample, point), label))\n",
        "\n",
        "  results.sort()\n",
        "  return results[0][1]\n",
        "\n",
        "\n",
        "'''testing for accuracy\n",
        "   Note: Runtime is around 2m 45s\n",
        "   Accuracy: 75%                 '''\n",
        "for i in range(len(tokenized_testset)):\n",
        "  sample=tokenized_testset[i]\n",
        "  correct_label=test_labels[i]\n",
        "  output=KNN(sample)\n",
        "\n",
        "  if output==correct_label:\n",
        "    total_num_correct=total_num_correct+1\n",
        "\n",
        "print(\"Accuracy:\",total_num_correct/number_of_testing_cases)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OKk3HGH7eX_"
      },
      "source": [
        "## Q2(b) (15 Points) Please print out three samples from the test set along with their nearest neighbor from the train set. Discuss your observations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Wx_CZyZ6--QY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e16db18-d2aa-49a2-b13e-15002203829c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first test sample is \n",
            "[101, 2054, 2003, 1996, 2918, 1997, 1996, 3863, 6165, 1029, 102]\n",
            "Its nearest neighbor is \n",
            "[101, 2054, 2003, 1996, 3120, 1997, 2115, 3863, 6165, 1029, 102]\n",
            "Its predicted label is: 32\n",
            "Its actual label is: 32\n",
            "\n",
            "The second test sample is \n",
            "[101, 2026, 10632, 2140, 2003, 2145, 14223, 1012, 2339, 1029, 102]\n",
            "Its nearest neighbor is \n",
            "[101, 2339, 2003, 2026, 10632, 2140, 2000, 5356, 2145, 14223, 1029, 102]\n",
            "Its predicted label is: 46\n",
            "Its actual label is: 46\n",
            "\n",
            "The third test sample is \n",
            "[101, 1045, 3477, 2098, 2007, 1037, 4003, 1998, 2001, 5338, 2019, 4469, 7408, 102]\n",
            "Its nearest neighbor is \n",
            "[101, 1045, 3477, 2098, 2007, 1996, 4003, 1998, 2045, 2001, 2019, 4469, 7408, 1029, 102]\n",
            "Its predicted label is: 15\n",
            "Its actual label is: 15\n",
            "\n",
            "Observation:\n",
            "All three sample labels appear to have been predicted correctly.\n",
            "This is not suprising since the our KNN algorithm has a 75% acuuracy rate, so\n",
            "it would have been harder to find a sample label that was not predicted correctly.\n",
            "One thing I noticed is that the samples dont always have the same size, as\n",
            "shown in second test, where the test sample has a size of 11, while its nearest\n",
            "neighbor has a size of 12. Another thing I noticed is that the samples and their \n",
            "nearest neighbor sometimes do not have elements at the same index. For example, \n",
            "the second sample shared the element '10632', but the element in the sample was at\n",
            "index 2, while the element at the nearest neighbor was at index 4. The order of \n",
            "which the elements reside must not matter since we used Jaccards distance for\n",
            "our KNN algorithm. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "'''Three samples'''\n",
        "sample1=tokenized_testset[100]\n",
        "sample2=tokenized_testset[200]\n",
        "sample3=tokenized_testset[800]\n",
        "\n",
        "'''This KNN function returns the nearest neighbor, while the previous KNN function returns the nearest neighbor's label'''\n",
        "def KNN_neighbor(sample):\n",
        "  results=[]\n",
        "  for i in range(len(tokenized_trainset)):\n",
        "    point=tokenized_trainset[i]\n",
        "    results.append((jaccard_distance(sample, point), point))\n",
        "\n",
        "  results.sort()\n",
        "  return results[0][1]\n",
        "\n",
        "'''sample 1'''\n",
        "print(\"The first test sample is \")\n",
        "print(sample1)\n",
        "print(\"Its nearest neighbor is \")\n",
        "print(KNN_neighbor(sample1))\n",
        "print(\"Its predicted label is: \", end=\"\")\n",
        "print(KNN(sample1))\n",
        "print(\"Its actual label is: \", end=\"\")\n",
        "print(test_labels[100])\n",
        "print()\n",
        "\n",
        "'''sample 2'''\n",
        "print(\"The second test sample is \")\n",
        "print(sample2)\n",
        "print(\"Its nearest neighbor is \")\n",
        "print(KNN_neighbor(sample2))\n",
        "print(\"Its predicted label is: \", end=\"\")\n",
        "print(KNN(sample2))\n",
        "print(\"Its actual label is: \", end=\"\")\n",
        "print(test_labels[200])\n",
        "print()\n",
        "\n",
        "'''sample 3'''\n",
        "print(\"The third test sample is \")\n",
        "print(sample3)\n",
        "print(\"Its nearest neighbor is \")\n",
        "print(KNN_neighbor(sample3))\n",
        "print(\"Its predicted label is: \", end=\"\")\n",
        "print(KNN(sample3))\n",
        "print(\"Its actual label is: \", end=\"\")\n",
        "print(test_labels[800])\n",
        "print()\n",
        "\n",
        "print(\"Observation:\")\n",
        "print(\"\"\"All three sample labels appear to have been predicted correctly.\n",
        "This is not suprising since the our KNN algorithm has a 75% acuuracy rate, so\n",
        "it would have been harder to find a sample label that was not predicted correctly.\n",
        "One thing I noticed is that the samples dont always have the same size, as\n",
        "shown in second test, where the test sample has a size of 11, while its nearest\n",
        "neighbor has a size of 12. Another thing I noticed is that the samples and their\n",
        "nearest neighbor sometimes do not have elements at the same index. For example,\n",
        "the second sample shared the element '10632', but the element in the sample was at\n",
        "index 2, while the element at the nearest neighbor was at index 4. The order of\n",
        "which the elements reside must not matter since we used Jaccards distance for\n",
        "our KNN algorithm.\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uDinHICAO-1"
      },
      "source": [
        "## Q3(a) (10 Points) Below, we have provided a method to generate embeddings. Please study the code and generate embeddings for the training set. Also complete the function to calculate eucleadian distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifu4sseCACqn",
        "outputId": "04e6e45e-99e9-4b32-e1d3-9e4bc9d1c0b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "testset_embeddings:\n",
            "tensor([[-0.0102,  0.0188, -0.0575,  ...,  0.0964, -0.0615, -0.0092],\n",
            "        [-0.0428, -0.0263,  0.0400,  ..., -0.0296, -0.0079,  0.0679],\n",
            "        [-0.0337,  0.0374,  0.0194,  ...,  0.0616, -0.0190,  0.0224],\n",
            "        ...,\n",
            "        [ 0.1096, -0.0527, -0.0050,  ..., -0.1261,  0.0224, -0.0335],\n",
            "        [ 0.0984, -0.0408,  0.0406,  ..., -0.0376,  0.0214, -0.0130],\n",
            "        [-0.0144,  0.0701, -0.0568,  ..., -0.0215,  0.0555,  0.0137]])\n",
            "trainset_embeddings:\n",
            "tensor([[-0.0354, -0.0421, -0.0028,  ..., -0.1048, -0.0466,  0.0028],\n",
            "        [ 0.0226, -0.0135,  0.0243,  ...,  0.0013, -0.0254,  0.0173],\n",
            "        [-0.0460, -0.0199, -0.0015,  ..., -0.0797,  0.0138,  0.0683],\n",
            "        ...,\n",
            "        [ 0.0324, -0.0624,  0.0013,  ...,  0.0347, -0.0022,  0.0190],\n",
            "        [-0.0045,  0.0190,  0.0331,  ..., -0.0260,  0.0395,  0.0020],\n",
            "        [ 0.0193, -0.0252,  0.0279,  ..., -0.0542,  0.0313,  0.0475]])\n"
          ]
        }
      ],
      "source": [
        "# Function to calculate Euclidean distance\n",
        "import math\n",
        "def Euclidean_distance(array1, array2):\n",
        "  '''\n",
        "  Compute the Euclidean distance between two arrays.\n",
        "  '''\n",
        "  total=0\n",
        "  for i in range(len(array1)):\n",
        "    total=total+pow(array1[i]-array2[i],2)\n",
        "\n",
        "  return math.sqrt(total)\n",
        "\n",
        "#Mean Pooling - Take attention mask into account for correct averaging\n",
        "def mean_pooling(model_output, attention_mask):\n",
        "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
        "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "\n",
        "\n",
        "# Sentences we want sentence embeddings for\n",
        "sentences = test_dataset[\"text\"]\n",
        "# Load model from HuggingFace Hub\n",
        "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Tokenize sentences\n",
        "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "# Compute token embeddings\n",
        "with torch.no_grad():\n",
        "    model_output = model(**encoded_input)\n",
        "\n",
        "# Perform pooling\n",
        "testset_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
        "\n",
        "# Normalize embeddings\n",
        "testset_embeddings = F.normalize(testset_embeddings, p=2, dim=1)\n",
        "\n",
        "print(\"testset_embeddings:\")\n",
        "print(testset_embeddings)\n",
        "\n",
        "'''\n",
        "Write your code here\n",
        "\n",
        "Used a smaller training set to avoid computation issues\n",
        "new training set size: 4000\n",
        "'''\n",
        "sentences = train_dataset[\"text\"]\n",
        "sentences= sentences[:1000]+sentences[3000:4000]+sentences[6000:7000]+sentences[9000:10000]\n",
        "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
        "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
        "with torch.no_grad():\n",
        "    model_output = model(**encoded_input)\n",
        "trainset_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
        "trainset_embeddings = F.normalize(trainset_embeddings, p=2, dim=1)\n",
        "print(\"trainset_embeddings:\")\n",
        "print(trainset_embeddings)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zljm1sR5Ay4k"
      },
      "source": [
        "## Q3(b) (10 Points) Please write a KNN classifier that uses the ***Euclidean*** distance between ***Embeddings*** of samples. Predict the labels for the test set and printout the accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Gr6yGLa2BmWI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e81c624-2742-4eba-8455-74f5168bd123"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.22\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "total_num_correct = 0\n",
        "number_of_testing_cases = 50\n",
        "\n",
        "'''Decreased the embedded training set even more to avoid computational issues'''\n",
        "train_accuracy=trainset_embeddings[:1000]\n",
        "\n",
        "def KNN_ecldn(sample):\n",
        "  results=[]\n",
        "  for i in range(len(train_accuracy)):\n",
        "    point=train_accuracy[i]\n",
        "    label=train_labels[i]\n",
        "    results.append((Euclidean_distance(sample, point), label))\n",
        "\n",
        "  results.sort()\n",
        "  return results[0][1]\n",
        "\n",
        "'''testing for accuracy\n",
        "   # of tests: 50\n",
        "   Note: Runtime is around 4m 54s\n",
        "   Accuracy: 22%                 '''\n",
        "\n",
        "'''selected 50 test embedded sets to avoid runtime issues when compared to running all the sets\n",
        "  Selected by random for fairness'''\n",
        "for i in range(50):\n",
        "  j=random.randint(0,999)\n",
        "  sample=testset_embeddings[j]\n",
        "  correct_label=test_labels[j]\n",
        "  output=KNN_ecldn(sample)\n",
        "\n",
        "  if output==correct_label:\n",
        "    total_num_correct=total_num_correct+1\n",
        "\n",
        "print(\"Accuracy:\",total_num_correct/number_of_testing_cases)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBvG_jAPB5lC"
      },
      "source": [
        "## Q3(c) (10 Points) Printout the same three samples that you used for Q2(b) along with their nearest neighbors found in Q3(b). Discuss your observations.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "DtIyRCE2CUz2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7cfb5d7-6f45-44f4-fa39-195321c6c431"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The first test sample is \n",
            "[101, 2054, 2003, 1996, 2918, 1997, 1996, 3863, 6165, 1029, 102]\n",
            "Its nearest neighbor is \n",
            "[101, 2006, 2054, 2079, 2017, 2918, 2115, 3863, 6165, 1029, 102]\n",
            "Its predicted label is: 32\n",
            "Its actual label is: 32\n",
            "\n",
            "The second test sample is \n",
            "[101, 2026, 10632, 2140, 2003, 2145, 14223, 1012, 2339, 1029, 102]\n",
            "Its nearest neighbor is \n",
            "[101, 2026, 10534, 2003, 14223, 1010, 2339, 1029, 102]\n",
            "Its predicted label is: 46\n",
            "Its actual label is: 46\n",
            "\n",
            "The third test sample is \n",
            "[101, 1045, 3477, 2098, 2007, 1037, 4003, 1998, 2001, 5338, 2019, 4469, 7408, 102]\n",
            "Its nearest neighbor is \n",
            "[101, 2339, 2038, 2026, 4003, 2042, 5338, 2019, 4469, 9044, 1029, 102]\n",
            "Its predicted label is: 34\n",
            "Its actual label is: 15\n",
            "\n",
            "Observation:\n",
            " The samples and nearest neigbors are printed out in their tokenized version instead of their \n",
            "embedded version to better analyze the observations. 2 out of 3 samples appear to have been predicted correctly for thier labels.\n",
            "This is surprising since our accuracy was shown to be low. However, the accuracy was \n",
            "only low because not all of the data was used for this prediction. The data sets had to\n",
            "be cut significantly to avoid computational issues, so in reality, if all the data was \n",
            "used properly, we would have a more appropriate accuracy reading. \n",
            "One thing I noticed is that the nearest neighbor for the samples are not the same nearest\n",
            "neighbor as the ones in Q2b. This must be because of our use of Euclidean distance instead\n",
            "of jaccard distance. The sample that was not predicted correctly was sample 3. This may have been\n",
            "due to the fact that this sample was closer to where we shortened our test, so possible data points\n",
            "were not used. \n",
            "\n"
          ]
        }
      ],
      "source": [
        "'''Three samples'''\n",
        "sample1=tokenized_testset[100]\n",
        "sample2=tokenized_testset[200]\n",
        "sample3=tokenized_testset[800]\n",
        "\n",
        "'''This KNN function returns the nearest neighbor, while the previous KNN function returns the nearest neighbor's label'''\n",
        "def KNN_neighbor_q3(sample):\n",
        "  results=[]\n",
        "  for i in range(len(train_accuracy)):\n",
        "    point=train_accuracy[i]\n",
        "    unembedded=tokenized_trainset[i]\n",
        "    results.append((Euclidean_distance(sample, point), unembedded))\n",
        "  results.sort()\n",
        "  return results[0][1]\n",
        "\n",
        "\n",
        "\n",
        "'''The samples are printed out in their tokenized version instead of their embedded version to better analysis the observations'''\n",
        "\n",
        "'''sample 1'''\n",
        "print(\"The first test sample is \")\n",
        "print(sample1)\n",
        "print(\"Its nearest neighbor is \")\n",
        "print(KNN_neighbor_q3(testset_embeddings[100]))\n",
        "print(\"Its predicted label is: \", end=\"\")\n",
        "print(KNN_ecldn(testset_embeddings[100]))\n",
        "print(\"Its actual label is: \", end=\"\")\n",
        "print(test_labels[100])\n",
        "print()\n",
        "\n",
        "'''sample 2'''\n",
        "print(\"The second test sample is \")\n",
        "print(sample2)\n",
        "print(\"Its nearest neighbor is \")\n",
        "print(KNN_neighbor_q3(testset_embeddings[200]))\n",
        "print(\"Its predicted label is: \", end=\"\")\n",
        "print(KNN_ecldn(testset_embeddings[200]))\n",
        "print(\"Its actual label is: \", end=\"\")\n",
        "print(test_labels[200])\n",
        "print()\n",
        "\n",
        "'''sample 3'''\n",
        "print(\"The third test sample is \")\n",
        "print(sample3)\n",
        "print(\"Its nearest neighbor is \")\n",
        "print(KNN_neighbor_q3(testset_embeddings[800]))\n",
        "print(\"Its predicted label is: \", end=\"\")\n",
        "print(KNN_ecldn(testset_embeddings[800]))\n",
        "print(\"Its actual label is: \", end=\"\")\n",
        "print(test_labels[800])\n",
        "print()\n",
        "\n",
        "print(\"Observation:\")\n",
        "print(\"\"\" The samples and nearest neigbors are printed out in their tokenized version instead of their\n",
        "embedded version to better analyze the observations. 2 out of 3 samples appear to have been predicted correctly for thier labels.\n",
        "This is surprising since our accuracy was shown to be low. However, the accuracy was\n",
        "only low because not all of the data was used for this prediction. The data sets had to\n",
        "be cut significantly to avoid computational issues, so in reality, if all the data was\n",
        "used properly, we would have a more appropriate accuracy reading.\n",
        "One thing I noticed is that the nearest neighbor for the samples are not the same nearest\n",
        "neighbor as the ones in Q2b. This must be because of our use of Euclidean distance instead\n",
        "of jaccard distance. The sample that was not predicted correctly was sample 3. This may have been\n",
        "due to the fact that this sample was closer to where we shortened our test, so possible data points\n",
        "were not used.\n",
        "\"\"\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}